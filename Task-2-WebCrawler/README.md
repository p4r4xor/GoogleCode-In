# WebCrawler made for a GCI task for fedora.

```
Usage: ./main.py website (-o output_file) (or) ./main.py website -o output_file
```

### Prevention

1) Putting the disallow rules into your robots.txt will stop your site showing up in the search engines. 

2) Watermarking your content, so that at least if it does get ripped we can point to the watermarks and claim ownership.

3) Present a hard CAPTCHA and only allow them to proceed once it has completed. If the user switches from pages quickly, give a CAPTCHA again.
